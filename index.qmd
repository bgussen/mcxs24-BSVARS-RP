---
title: "Dixie Effect on Economic Stability in Australia"

author: "Ben Gussen"

execute:
  echo: false
  
bibliography: references.bib
---

```{r reseting and ensuring preporducability of random variables}
rm(list=ls())
set.seed(1234567)
```


```{r libraries, include=FALSE}
#| echo: false
#| message: false
#| warning: false
library(readrba)
library(dplyr)
library(zoo)
library(lubridate)
library(tseries)
library(ggplot2)        # For IRF plots
library(urca)           # For conducting the ADF test
library(forecast)       # For ACF and PACF plots 
library(gridExtra)
library(kableExtra)
library(MCMCpack)       # For Bayesian estimation
library(coda)           # For MCMC analysis
library(mvtnorm)        # For multivariate normal distributions
library(Matrix)         # For matrix operations
library(matrixcalc)     # For matrix calculations
library(BVAR)
library(MASS)           # For multivariate normal distributions
library(sn)             # For skew-normal and generalized normal distributions
library(parallel) 
library(actuar)         # For sampling for an inverse gamma distribution
library(plot3D)         # For the IRF plots
library(HDInterval)     # For the IRF plots
library(invgamma)       # For the extension models
library(quantreg)       # For the extension models
library(dplyr)    # For data manipulation
library(tidyr)    # For data manipulation

```

```{r the iterations for estimating B0 & Bp}

#Define global variables

  S_burnin <- 200      # number of discarded iterations from the Gibbs sampler
  S <- 500             # number of iterations used to calculate B0 and Bp
  p <- 12               # number of lags in the Baysian SVAR model 
  N <- 4               # number of variables in the Bayesian SVAR model
  K <-  1+N*p

```

```{r L16 code for useful functions}

# useful functions
############################################################
orthogonal.complement.matrix.TW = function(x){
  # x is a mxn matrix and m>n
  # the function returns a mx(m-n) matrix, out, that is an orthogonal complement of x, i.e.:
  # t(x)%*%out = 0 and det(cbind(x,out))!=0
  N     = dim(x)
  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)
  out   = as.matrix(tmp[,(N[2]+1):N[1]])
  return(out)
}

r.conditional.generalized.normal = function(S.inv, nu, Vn, n, B0){
  # A function to sample a random draw from a conditional generalized normal distribution
  # of the unrestricted elements of the n-th row of matrix B0 
  # given the parameters from the remaining rows of B0
  # Depends on package mvtnorm
  # use: library(rmvtnorm)
  
  rn            = nrow(Vn)
  Un            = chol(nu*solve(Vn%*%S.inv%*%t(Vn)))
  w             = t(orthogonal.complement.matrix.TW(t(B0[-n,])))
  w1            = w %*% t(Vn) %*% t(Un) / sqrt(as.numeric(w %*% t(Vn) %*% t(Un) %*% Un %*% Vn %*% t(w)))
  if (rn>1){
    Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))
  } else {
    Wn          = w1
  }
  alpha         = rep(NA,rn)
  u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))
  alpha[1]      = sqrt(as.numeric(u%*%t(u)))
  if (runif(1)<0.5){
    alpha[1]    = -alpha[1]
  }
  if (rn>1){
    alpha[2:rn] = rmvnorm(1,rep(0,nrow(Vn)-1),(1/nu)*diag(rn-1))
  }
  bn            = alpha %*% Wn %*% Un
  B0n           = bn %*% Vn
  
  output        = list(bn=bn, B0n=B0n)
  return(output)
}

rgn             = function(n,S.inv,nu,V,B0.initial){
  # This function simulates draws for the unrestricted elements 
  # of the conteporaneous relationships matrix of an SVAR model
  # from a generalized-normal distribution according to algorithm 
  # by Waggoner & Zha (2003, JEDC)
  # n     - a positive integer, the number of draws to be sampled
  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution
  # nu    - a positive scalar, degrees of freedom parameter
  # V     - an N-element list, with fixed matrices
  # B0.initial - an NxN matrix, of initial values of the parameters
  
  N             = nrow(B0.initial)
  no.draws      = n
  
  B0            = array(NA, c(N,N,no.draws))
  B0.aux        = B0.initial
  
  for (i in 1:no.draws){
    for (n in 1:N){
      rn            = nrow(V[[n]])
      Un            = chol(nu*solve(V[[n]]%*%S.inv%*%t(V[[n]])))
      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))
      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
      if (rn>1){
        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))
      } else {
        Wn          = w1
      }
      alpha         = rep(NA,rn)
      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))
      alpha[1]      = sqrt(as.numeric(u%*%t(u)))
      if (runif(1)<0.5){
        alpha[1]    = -alpha[1]
      }
      if (rn>1){
        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))
      }
      bn            = alpha %*% Wn %*% Un
      B0.aux[n,]    = bn %*% V[[n]]
    }
    B0[,,i]         = B0.aux
  }
  
  return(B0)
}

normalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){
  # This function normalizes a matrix of contemporaneous effects
  # according to the algorithm by Waggoner & Zha (2003, JOE)
  # B0        - an NxN matrix, to be normalized
  # B0.hat    - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0)
  K                 = 2^N
  distance          = rep(NA,K)
  for (k in 1:K){
    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)
    distance[k]     = sum(
      unlist(
        lapply(1:N,
               function(n){
                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]
               }
        )))
  }
  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0
  
  return(B0.out)
}

normalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){
  # This function normalizes the Gibbs sampler output from function rgn
  # using function normalization.wz2003 
  # B0.posterior  - a list, output from function rgn
  # B0.hat        - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0.hat)
  K                 = 2^N
  
  B0.hat.inv        = solve(B0.hat)
  Sigma.inv         = t(B0.hat)%*%B0.hat
  
  diag.signs        = matrix(NA,2^N,N)
  for (n in 1:N){
    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))
  }
  
  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){
    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)
  },mc.cores=4
  )
  B0.posterior.n  = simplify2array(B0.posterior.n)
  
  return(B0.posterior.n)
}

normalize.Gibbs.output          = function(B0.posterior,B0.hat){
  # This function normalizes the Gibbs sampler output from function rgn
  # using function normalization.wz2003 
  # B0.posterior  - a list, output from function rgn
  # B0.hat        - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0.hat)
  K                 = 2^N
  
  B0.hat.inv        = solve(B0.hat)
  Sigma.inv         = solve(B0.hat.inv %*% t(B0.hat.inv))
  
  diag.signs        = matrix(NA,2^N,N)
  for (n in 1:N){
    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))
  }
  
  for (i in 1:dim(B0.posterior)[3]){
    if (i%%100==0){ cat(i," ")}
    norm.post                   = normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)
    B0.posterior[,,i]           = norm.post
  }
  return(B0.posterior)
}

rnorm.ngn       = function(B0.posterior,B,Omega){
  # This function simulates draws for the multivariate normal distribution
  # of the autoregressive slope matrix of an SVAR model
  # from a normal-generalized-normal distribution according to algorithm 
  # by Waggoner & Zha (2003, JEDC)
  # B0.posterior  - a list, output from function rgn
  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0
  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution
  
  N             = nrow(B)
  K             = ncol(B)
  no.draws      = dim(B0.posterior)[3]
  L             = t(chol(Omega))
  
  Bp.posterior  = lapply(1:no.draws,function(i){
    Bp          = matrix(NA, N, K)
    for (n in 1:N){
      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))
    }
    return(Bp)
  })
  Bp.posterior  = simplify2array(Bp.posterior)
  return(Bp.posterior)
}

```

```{r function to create the Y and X matrices for the Bayesian SVAR model}

create_matrices <- function() {
  # Assuming USD_Index, CPI_rate, unemp_rate, and GDP_rate are available in the global environment

  # Combining the data into one data frame
  data <- data.frame(
    USD_Index = USD_Index$value, 
    Inflation = CPI_rate$value, 
    Unemployment = unemp_rate$value, 
    GDP_growth = GDP_rate$value
  )

  # Convert to a matrix Y
  Y <- as.matrix(data)

  # Define the number of lags
  lags <- p

  # Manually shift the matrix to create lagged X
  X <- Y[1:(nrow(Y) - lags), ]  # Get all rows up to the last 'lags' rows

  # Adjust Y to match the new dimensions of X
  Y <- Y[(lags + 1):nrow(Y), ]  # Start from row 'lags+1' to end

  # Returning both matrices
  return(list(X = X, Y = Y))
}


```

```{r function to generate poseriors for the random walk}

generate_posteriors <- function(Y, V_matrices, kappa1, kappa2, kappa3, kappa4) {

  y   = Y
  
  # Basic settings and model dimensions
  NN <- ncol(y)
  pp <- 1   # This is the number of lags in the model
  K <- 1 + NN * pp


  
  # Set the priors
  #kappa1 <- 0.01      # autoregressive slope shrinkage
  #kappa2 <- 1         # constant term shrinkage 
  #kappa3 <- 100       # contemporaneous effects shrinkage
  #kappa4 <- 1         # VAR prior persistence
   

# create Y and X
############################################################
Y       = y[(pp+1):nrow(y),]
X       = matrix(1,nrow(Y),1)
for (i in 1:pp){
  X     = cbind(X,y[((pp+1):nrow(y))-i,])
}

Y       = t(Y)
X       = t(X)
  
 
  priors <- list(
    B = cbind(rep(0, NN), kappa4 * diag(NN), matrix(0, NN, (pp - 1) * NN)),
    Omega = diag(c(kappa2, kappa1 * ((1:pp)^(-2)) %x% rep(1, NN))),
    S = kappa3 * diag(NN),
    nu = NN
  )
  
  # Compute posterior distribution parameters
  Omega.inv <- solve(priors$Omega)
  Omega.post.inv <-  (X %*% t(X) +  Omega.inv)
  Omega.post <- solve(Omega.post.inv)
  B.post <-   (Y %*% t(X) +  priors$B %*% Omega.inv) %*% Omega.post
  S.post <-  solve(Y %*% t(Y) +  solve(priors$S) + priors$B %*% Omega.inv %*% t(priors$B) - B.post %*% Omega.post.inv %*% t(B.post))
  nu.post <- nrow(y) + priors$nu
  
  posteriors <- list(
    B = B.post,
    Omega = Omega.post,
    S = S.post,
    nu = nu.post
  )
  
  # Initialize B0
  B0_initial <- matrix(0, NN, NN)
  for (n in 1:NN) {
    unrestricted <- apply(V_matrices[[n]], 2, sum) == 1
    B0_initial[n, unrestricted] <- rnorm(sum(unrestricted))
  }
  
  # Sampling B0 from the posterior using Gibbs sampling
  B0_posterior <- rgn(n = S_burnin, S.inv = solve(S.post), nu = nu.post, V = V_matrices, B0.initial = B0_initial)
  B0_posterior <- rgn(n = S, S.inv = solve(S.post), nu = nu.post, V = V_matrices, B0.initial = B0_posterior[,,S_burnin])
  
  # Normalization
  B0_hat <- diag(sign(diag(B0_posterior[,,S]))) %*% B0_posterior[,,S]
  B0_posterior <- normalize.Gibbs.output.parallel(B0_posterior, B0.hat = B0_hat)
  
  # Sample B+ from the normal conditional posterior
  Bp_posterior <- rnorm.ngn(B0.posterior = B0_posterior, B = B.post, Omega = Omega.post)
  
  # Return posteriors
  return(list(B0 = B0_posterior, Bp = Bp_posterior))
}
```



```{r function to estimate the Basic Bayesian SVAR model}

estimate_basic_model <- function (S_burnin, S){
  
  matrices <- create_matrices()
  Y = matrices$Y

  y   = Y
   
     # Basic settings and model dimensions
  N <- ncol(y)
  p <- 12   # This is the number of lags in the model
  K <- 1 + N * p

# create Y and X
############################################################
Y       = y[(p+1):nrow(y),]
X       = matrix(0,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[((p+1):nrow(y))-i,])
}

Y       = t(Y)
X       = t(X)


V <- list()
  V[[1]] <- matrix(c(1, 0, 0, 0), nrow = 1, ncol = 4, byrow = TRUE)
  V[[2]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0), nrow = 2, ncol = 4, byrow = TRUE)
  V[[3]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0), nrow = 3, ncol = 4, byrow = TRUE)
  V[[4]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0,
                     0, 0, 0, 1), nrow = 4, ncol = 4, byrow = TRUE)


# Set the priors
kappa1 <- 0.01      # autoregressive slope shrinkage
kappa2 <- 1         # constant term shrinkage 
kappa3 <- 100       # contemporaneous effects shrinkage
kappa4 <- 1         # VAR prior persistence

# Basic settings and model dimensions
N <- ncol(y)
p <- 12   # This is the number of lags in the model
K <- 1 + N * p

priors <- list(
  B = cbind(rep(0, N), kappa4 * diag(N), matrix(0, N, (p - 1) * N)),
  Omega = diag(c(kappa2, kappa1 * ((1:p)^(-2)) %x% rep(1, N))),
  S = kappa3 * diag(N),
  nu = N
)

# Initialize B0
B0.initial <- matrix(0, N, N)
for (n in 1:N) {
  unrestricted <- apply(V[[n]], 2, sum) == 1
  B0.initial[n, unrestricted] <- rnorm(sum(unrestricted))
}

N = nrow(B0.initial)
no.draws = S_burnin

B0 = array(NA, c(N, N, no.draws))
Bp = array(NA, c(nrow(priors$B), ncol(priors$B), no.draws))

B0.aux = B0.initial
Bp.aux = matrix(NA, nrow(priors$B), ncol(priors$B))

for (i in 1:S_burnin) {
  # Compute posterior distribution parameters
  Omega.inv <- solve(priors$Omega)
  Omega.post.inv <- (X %*% t(X) + Omega.inv)
  Omega.post <- solve(Omega.post.inv)
  B.post <- (Y %*% t(X) + priors$B %*% Omega.inv) %*% Omega.post
  S.post <- solve(Y %*% t(Y) + solve(priors$S) + priors$B %*% Omega.inv %*% t(priors$B) - B.post %*% Omega.post.inv %*% t(B.post))
  nu.post <- nrow(y) + priors$nu
  
  # Check if Omega.post and S.post is positive definite and store the inverse if it is
  is_positive_definite <- tryCatch({
    chol(Omega.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  is_positive_definite <- tryCatch({
    chol(S.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }
  
  S.inv <- solve(S.post)
  nu = nu.post

  for (n in 1:nrow(B0.initial)) {
    rn = nrow(V[[n]])
    Un = chol(nu * solve(V[[n]] %*% S.inv %*% t(V[[n]])))
    w = t(orthogonal.complement.matrix.TW(t(B0.aux[-n, ])))
    w1 = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
    if (rn > 1) {
      Wn = cbind(t(w1), orthogonal.complement.matrix.TW(t(w1)))
    } else {
      Wn = w1
    }
    alpha = rep(NA, rn)
    u = rmvnorm(1, rep(0, nu + 1), (1 / nu) * diag(nu + 1))
    alpha[1] = sqrt(as.numeric(u %*% t(u)))
    if (runif(1) < 0.5) {
      alpha[1] = -alpha[1]
    }
    if (rn > 1) {
      alpha[2:rn] = rmvnorm(1, rep(0, nrow(V[[n]]) - 1), (1 / nu) * diag(rn - 1))
    }
    bn = alpha %*% Wn %*% Un
    B0.aux[n, ] = bn %*% V[[n]]
    
    K = ncol(B.post)
    L = t(chol(Omega.post))
    
    Bp.aux[n, ] = as.vector(t(B0.aux[n, ] %*% B.post) + L %*% rnorm(K))
  }
  
  B0[, , i] = B0.aux
  Bp[, , i] = Bp.aux

}

B0_posterior <- B0

# Initialize B0 again for the remainder of the iterations 
B0.initial <- B0_posterior[, , S_burnin]
 
N = nrow(B0.initial)
no.draws = S

B0 = array(NA, c(N, N, no.draws))
Bp = array(NA, c(nrow(priors$B), ncol(priors$B), no.draws))

B0.aux = B0.initial
Bp.aux = matrix(NA, nrow(priors$B), ncol(priors$B))

for (j in 1:S) {
  # Compute posterior distribution parameters
  Omega.inv <- solve(priors$Omega)
  Omega.post.inv <- (X %*% t(X) + Omega.inv)
  Omega.post <- solve(Omega.post.inv)
  B.post <- (Y %*% t(X) + priors$B %*% Omega.inv) %*% Omega.post
  S.post <- solve(Y %*% t(Y) + solve(priors$S) + priors$B %*% Omega.inv %*% t(priors$B) - B.post %*% Omega.post.inv %*% t(B.post))
  nu.post <- nrow(y) + priors$nu

  # Check if Omega.post and S.post is positive definite and store the inverse if it is
  is_positive_definite <- tryCatch({
    chol(Omega.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  is_positive_definite <- tryCatch({
    chol(S.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  S.inv <- solve(S.post)
  nu = nu.post

  for (n in 1:N) {
    rn = nrow(V[[n]])
    Un = chol(nu * solve(V[[n]] %*% S.inv %*% t(V[[n]])))
    w = t(orthogonal.complement.matrix.TW(t(B0.aux[-n, ])))
    w1 = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
    if (rn > 1) {
      Wn = cbind(t(w1), orthogonal.complement.matrix.TW(t(w1)))
    } else {
      Wn = w1
    }
    alpha = rep(NA, rn)
    u = rmvnorm(1, rep(0, nu + 1), (1 / nu) * diag(nu + 1))
    alpha[1] = sqrt(as.numeric(u %*% t(u)))
    if (runif(1) < 0.5) {
      alpha[1] = -alpha[1]
    }
    if (rn > 1) {
      alpha[2:rn] = rmvnorm(1, rep(0, nrow(V[[n]]) - 1), (1 / nu) * diag(rn - 1))
    }
    bn = alpha %*% Wn %*% Un
    B0.aux[n, ] = bn %*% V[[n]]

    K = ncol(B.post)
    L = t(chol(Omega.post))

    Bp.aux[n, ] = as.vector(t(B0.aux[n, ] %*% B.post) + L %*% rnorm(K))
  }

  B0[ , , j] = B0.aux
  Bp[ , , j] = Bp.aux


}

B0_posterior <- B0
Bp_posterior <- Bp

# Normalization
B0_hat <- diag(sign(diag(B0_posterior[, , S]))) %*% B0_posterior[, , S]

B0_posterior <- normalize.Gibbs.output.parallel(B0_posterior, B0.hat = B0_hat)

# Sample B+ from the normal conditional posterior
#Bp_posterior2 <- Bp_posterior 
Bp_posterior <- rnorm.ngn(B0.posterior = B0_posterior, B = B.post, Omega = Omega.post)

return(list(B0_posterior = B0_posterior, Bp_posterior = Bp_posterior))

}

```

```{r Function for plotting Bp lagged parameters}

plot_lagged_effects <- function(B0_posterior, Bp_posterior) {

# Define lagged values for each variable
lagged_values <- seq(2, 46, by = 4)

# Plotting setup
plot_title <- "Effect of lagged USD_Index on Australian Economic Stability"
plot(x = NA, y = NA, xlim = c(1, length(lagged_values)), ylim = c(-2, 2), type = "n", xlab = "Lag Number", ylab = "Effect", main = plot_title)

# Define variable indices and colors for plotting
variables <- list(inflation = 2, unemployment = 3, gdp = 4)
colors <- c("blue", "green", "red")
names(colors) <- names(variables)

# Loop through each variable and plot
for (var_name in names(variables)) {
  var_index <- variables[[var_name]]  # Get the correct index for each variable
  ratio_vector <- numeric(length(lagged_values))  # Initialize vector to store ratio values
  for (i in seq_along(lagged_values)) {
    j <- lagged_values[i]
    # Calculate ratio for the current variable using Bp.posterior2 and B0.posterior2
    ratio_vector[i] <- Bp_posterior[var_index, j, S] / B0_posterior[var_index, var_index, S]
  }
  lines(1:length(lagged_values), ratio_vector, col = colors[var_name])  # Plot the accumulated ratios using simple lag numbers
}

# Add legend
legend("topright", legend = names(variables), col = colors, lty = 1, title = "Variables")

}

```

```{r function to calculate and print the Standard Deviation of B0}

calculate_and_print_B0sd <- function(B0_posterior) {
  # Calculate the standard deviation of B0.posterior along the desired axes
  B0_sd <- apply(B0_posterior, c(1, 2), sd)

  # Print the standard deviation matrix with a title
  cat("Standard deviation of the model B0:\n")
  print(B0_sd)
}

```

```{r function to calculate and print the Standard Deviation of Bp}

calculate_and_print_Bpsd <- function(Bp_posterior) {
# Calculate the standard deviation of Bp.posterior along the desired axis
Bp_sd <- apply(Bp_posterior, c(1, 2), sd)

print("Standard deviation of the model Bp")

# Print or use the standard deviation matrix as needed
print(Bp_sd)
}

```

```{r Function to calculate and represent the significance levels for B0_posterior}

# Function to calculate significance of the last matrix elements against zero, with NA handling
calculate_significance <- function(matrix_posterior) {
  n_vars <- dim(matrix_posterior)[1]
  n_periods <- dim(matrix_posterior)[2]
  n_draws <- dim(matrix_posterior)[3]
  
  # Get the last matrix
  last_matrix <- matrix_posterior[, , n_draws]
  
  # Initialize matrix to store the significance symbols
  significance_matrix <- matrix("", nrow = n_vars, ncol = n_periods)
  
  # Loop over each element of the matrix
  for (i in 1:n_vars) {
    for (j in 1:n_periods) {
      # Calculate standard deviation for each element across all draws
      element_std_dev <- sd(matrix_posterior[i, j, ], na.rm = TRUE)
      
      # Get the value from the last matrix
      last_value <- last_matrix[i, j]
      
      # Check if standard deviation is zero or NA - which would lead to NA in z-score
      if (is.na(element_std_dev) || element_std_dev == 0) {
        significance_matrix[i, j] <- "NA"  # Indicate non-applicable significance due to constant/NA values
      } else {
        # Calculate the z-score assuming mean is zero for the null hypothesis
        z_score <- last_value / element_std_dev
        
        # Determine significance based on z-score (typical critical z values for two-tailed tests)
        if (abs(z_score) > 1.64) {  # Approximately 10% significance level (two-tailed)
          significance_matrix[i, j] <- "*"
        }
        if (abs(z_score) > 1.96) {  # 5% significance level (two-tailed)
          significance_matrix[i, j] <- "**"
        }
        if (abs(z_score) > 2.58) {  # 1% significance level (two-tailed)
          significance_matrix[i, j] <- "***"
        }
      }
    }
  }
  
  return(significance_matrix)
}

```

```{r function to plot the IRF responses to a one-standard-deviation negative shock to the USD_Index}

plot_irfs <- function(B0, Bp, horizon, conf_level) {
  # Initialize IRF array
  IRFs <- array(NA, dim = c(N, horizon + 1, S))

  # Extract the USD_Index column from Y matrix and calculate its standard deviation
  
  matrices <- create_matrices()
  Y = matrices$Y
  usd_index <- Y[, 1]
  sd_usd_index <- sd(usd_index)
  
  # Define the shock vector (negative 1 SD shock to the USD_Index)
  shock_vector <- matrix(c(-sd_usd_index, rep(0, N - 1)), nrow = N, ncol = 1)

  # Calculate IRFs for each posterior draw
  for (s in 1:S) {
    B0_inv <- solve(B0[,,s])  # Inverse of B0 for simulation s
    current_shock <- B0_inv %*% shock_vector  # Initial impact of the shock
    
    for (t in 1:(horizon + 1)) {
      IRFs[,t,s] <- current_shock
      if (t < horizon + 1) {
        # Construct lagged response vector (initially filled with zeros for future values)
        response_vector <- matrix(0, N * p + 1, 1) # +1 for intercept
        response_vector[1:N, 1] <- current_shock
        # Apply lagged effects
        current_shock <- B0_inv %*% matrix(Bp[,,s] %*% response_vector, nrow = N)
      }
    }
  }

  # Compute median and confidence intervals
  IRF_median <- apply(IRFs, 1:2, median)
  IRF_lower <- apply(IRFs, 1:2, quantile, probs = (1 - conf_level) / 2)
  IRF_upper <- apply(IRFs, 1:2, quantile, probs = 1 - (1 - conf_level) / 2)

  # Corrected Data frame for plotting
  data_to_plot <- data.frame(
    Time = rep(0:horizon, N),
    IRF = as.vector(t(IRF_median)),
    Lower = as.vector(t(IRF_lower)),
    Upper = as.vector(t(IRF_upper)),
    Variable = rep(c("USD_Index", "Inflation", "Unemployment", "GDP_growth"), each = horizon + 1)
  )

  # Determine the overall range for y-axis limits
  global_y_min <- min(data_to_plot$Lower)
  global_y_max <- max(data_to_plot$Upper)

  # Create plots for each variable
  plots <- lapply(c("USD_Index", "Inflation", "Unemployment", "GDP_growth"), function(var) {
    ggplot(subset(data_to_plot, Variable == var), aes(x = Time, y = IRF)) +
    geom_ribbon(aes(ymin = Lower, ymax = Upper, fill = Variable), alpha = 0.2) +
    geom_line() +
    scale_y_continuous(limits = c(global_y_min, global_y_max)) +
    labs(title = var) +
    theme_minimal()
  })

  # Arrange the plots in a grid
  grid_arrange <- do.call(gridExtra::grid.arrange, c(plots, ncol = 2))
  
  invisible(grid_arrange)
}

```


> **Abstract.** This research project investigates the dynamic effects of the U.S. Dollar (USD) on the Australian economy, with a focus on economic stability. By employing Bayesian Structural Vector Autoregression (SVAR) analysis. This study aims to elucidate the transmission mechanisms of USD fluctuations through three channels: GDP growth, inflation, and unemployment. The findings are expected to provide nuanced insights into the macroeconomic interdependencies between the USD and the Australian economy, offering valuable perspectives for policymakers and economic analysts.
>
> **Keywords.** Bayesian SVAR, USD, Australian economy, inflation, economic stability

### Introduction

This document presents the design and implementation of a Bayesian Structural Vector Autoregression (SVAR) model. Our focus is on understanding the impact of the Trade Weighted Index (TWI) of the USD on Australian economic indicators: unemployment, inflation, and GDP rate.

### Research Proposal Structure

#### The Question, Objective, and Motivation

**Objective:** To examine the dynamic effects of a weakening USD on the Australian economy, focusing on changes to the GDP, inflation and unemployment.

**Research Question:** How does a weakening United States Dollar (USD) influence overall economic stability in Australia?

**Motivation:** The relationship between the USD and the Australian economy is crucial, given the extensive trade links and financial interactions between the two nations. In light of recent global financial uncertainties, understanding this interplay is essential for crafting informed economic policies and strategies. This research endeavors to dissect the complexities of this economic relationship, aiming to provide insights that could inform both policymakers and market participants.

#### The US dollar trade weighted index or the USD/AUD exchange rate?

@Chen2004 explores the impact of exchange rate movements on firm values in New Zealand, suggesting broader macroeconomic implications. Extending this analysis, @Chen2024 discusses the U.S. Dollar Index (USDX, DXY, "Dixie"), which measures the U.S. dollar's strength against a basket of major trading partner currencies. This index is vital for understanding the U.S. dollar's overall economic health and its global trade competitiveness.

Using the USD Index is preferred over the USD/AUD exchange rate because it provides a more comprehensive view of the US dollar's strength by comparing it against a basket of currencies from major trading partners. This broader perspective reflects overall trade competitiveness and economic impacts more accurately than the USD/AUD rate, which only measures the exchange value between two currencies. The USD Index, therefore, offers a better gauge of the US dollar's performance on a global scale, making it more suitable for analyzing its effect on the economic health of the Australian economy.

#### What is economic stability?

Economic stability indicates a nation's economy is in a healthy state, characterized by low and stable inflation, allowing for future financial planning without significant loss of purchasing power. It also involves a moderate unemployment rate, ensuring sufficient employment opportunities without causing wage inflation due to a limited labor pool. Additionally, the economy should experience steady growth with minor fluctuations in output, avoiding major booms and busts. Governments and central banks strive to maintain this stability using fiscal policies, such as taxation and spending, and monetary policies, including adjusting interest rates. Achieving economic stability is challenging as it requires maintaining a balance across various economic indicators. This stability is crucial as it enhances business confidence, boosts consumer spending, and supports overall economic health, with metrics like inflation, unemployment, and GDP growth serving as indicators of economic stability.

#### Effect of a weak US dollar on the Australian economy

A weaker US Dollar Index typically strengthens the Australian dollar, impacting the Australian economy in several ways:

1. **Inflation:** A stronger Australian dollar makes exports less competitive and imports cheaper, potentially decreasing export volumes while reducing inflation through lower import costs.

2. **Unemployment:** Reduced foreign investment due to a stronger Australian dollar could slow economic growth and potentially increase unemployment if businesses scale back expansion.

3. **GDP Growth:** The trade balance may suffer if reduced export revenues outweigh the benefits of cheaper imports, negatively impacting GDP growth.

### Data and Their Properties

In order to conduct a thorough analysis of the impact of USD fluctuations on the Australian economy, this study will utilize a set of time-series data that encapsulates three economic indicators:

- **Growth Rate in the Australian Gross Domestic Product:** Quarterly GDP figures representing the overall economic activity within Australia.

- **Unemployment Rate:** Percentage of the labor force that is currently unemployed and actively seeking employment.

- **Inflation Rate:** Measured by the change in the Consumer Price Index (CPI), reflecting the changes in prices for goods and services.

These indicators will be analyzed against one proxy for the strength of the USD dollar as an international currency. 

- **USD Trade Weighted Index:** monthly index of US dollar major currency trade-weighted index. 


#### Motivation for Data Choice:

The selected variables offer a detailed insight into the economic interactions between the US dollar and the Australian economy, focusing on how trade-weighted indices affect the competitiveness of Australian goods and services internationally. Changes in the US dollar's strength can significantly impact Australia's GDP growth, given the substantial trade relations with the United States. Additionally, fluctuations in this index are likely to alter inflation and unemployment rates, key indicators of economic stability that affect purchasing power and consumer spending. This comprehensive approach enables a thorough examination of the international economic dynamics and the overall stability of the Australian economy.

### Data Acquisition and Transformation:

The project will analyze data from 1980 to 2019 sourced from official Australian databases, particularly the Reserve Bank of Australia, focusing on the effects of a weakening USD on the Australian economy. Data from before 1980 and post-2019 are excluded due to their irrelevance to current trade relations and distortions from the COVID-19 crisis, respectively. The analysis will use time series data at different frequencies within a Bayesian Structural Vector Autoregression (SVAR) model, requiring preprocessing to match data frequencies.

For our purposes in this initial proposal, the required data will be sourced from official Australian government databases using the `readrba` R packages. Based on the preliminary results from the project, it might become necessary to entertain a mixed frequency approach. 

The first series, the year-end Australian real GDP growth rate, is from the Reserve Bank of Australia (RBA) output and labour statistical tables [@RBA2024a]. This is a quarterly series, and will not need to be treated for frequency alignment. 


```{r GDP growth rate}
#| echo: false
#| message: false
#| warning: false
#| label: fig-GDP
#| fig-cap: "ACF and PACF functions for the GDP growth rate"

# Downloading data from the internet
series <- read_rba(series_id = "GGDPCVGDPY")

#clean the series by removing missing values
series <- na.omit(series)

# Ensuring the date format is appropriate for quarterly data
series <- series %>% 
  mutate(date = as.yearqtr(date, format = "%Y-%q"))

# Filtering the dataset for the desired date range
GDP_rate <- series %>%
  filter(date >= as.yearqtr("1980 Q1") & date <= as.yearqtr("2019 Q4"))

# Set up the plotting area

par(mfrow=c(1,2))

# Plot ACF with lags limited to 10
acf(GDP_rate$value, main = "ACF for GDP Rate", lag.max = 10)
# Plot PACF with lags limited to 10
pacf(GDP_rate$value, main = "PACF for GDP Rate", lag.max = 10)
# Reset the plotting area back to default
par(mfrow=c(1, 1))
```


As seen in @fig-GDP, the PACF above, the optimal lag for conducting the augmented Dicky-Fuller (DF) test is 4. A summary of the results of this test can be found in the table below. 


```{r ADF test for DGP_rate}
#| echo: false
#| message: false
#| warning: false

# Conduct the ADF test to check for stationarity
adf_test_result <- ur.df(GDP_rate$value, type = "drift", lags = 4, selectlags = "AIC")
```

Note that the augmented DF test suggests that the series is stationary at the 95% level. There is no need to modify the series. From the DF test results, its is safe to conclude that the GDP_rate series is stationary. 

The second series, the year-end change in Australian inflation rate, is from the Reserve Bank of Australia (RBA) inflation and inflation expectations statistical tables (@RBA2024b). This series has been converted from a monthly series into a quarterly series by choosing the relevant months for the beginning of each quarter, starting from March and ending with December.

```{r CPI rate}
#| echo: false 
#| message: false
#| warning: false
#| label: fig-CPI
#| fig-cap: "ACF and PACF functions for the CPI rate"

## Downloading CPI data from the internet
series <- read_rba(series_id = "GCPIAGYP") 

#clean the series by removing missing values
series <- na.omit(series)

# Ensuring the date format is appropriate for quarterly data
series <- series %>% 
  mutate(date = as.yearqtr(date, format = "%Y-%q"))

# Ensuring that date is recognized as a quarterly object and summarizing to quarterly values
series_quarterly <- series %>% 
  mutate(quarter = as.yearqtr(date)) %>%
  group_by(quarter) %>%
  summarize(value = mean(value, na.rm = TRUE)) %>%
  ungroup()

# Filter for the correct range from Q1 1980 to Q4 2019
CPI_rate <- series_quarterly %>%
  filter(quarter >= as.yearqtr("1980 Q1") & quarter <= as.yearqtr("2019 Q4"))

# Set up the plotting area
par(mfrow=c(1, 2))

# Plot ACF with lags limited to 10
acf(CPI_rate$value, main = "ACF for CPI Rate", lag.max = 10)

# Plot PACF with lags limited to 10
pacf(CPI_rate$value, main = "PACF for CPI Rate", lag.max = 10)

# Reset the plotting area back to default
par(mfrow=c(1,1))

# Conduct the ADF test to check for stationarity
adf_test_result <- ur.df(CPI_rate$value, type = "drift", lags = 4, selectlags = "AIC")

```

From @fig-CPI, an augmented DF test was conducted with 3 lags. Note that the augmented DF test suggests that the series is not stationary at the 95% confidence level. There was a need to take the difference of the series.

The third series, the Australian unemployment rate, is from the Reserve Bank of Australia (RBA) output and labour Statistical tables [@RBA2024c]. This is  a monthly series. The series was converted into a quarterly series by choosing the relevant months for the beginning of each quarter, starting from March and ending with December. 

```{r unemp_rate}
#| echo: false 
#| message: false
#| warning: false
#| label: fig-unemp
#| fig-cap: "ACF and PACF functions for the unemployment rate"

## Downloading unemployment data from the internet
series <- read_rba(series_id = "GLFSURSA") 

#clean the series by removing missing values
series <- na.omit(series)

# Convert the date format from dd/mm/yyyy to Date object
series <- series %>% 
  mutate(date = as.Date(as.character(date), format = "%Y-%m-%d")) 

# Ensuring that date is recognized as a quarterly object and summarizing to quarterly values
series_quarterly <- series %>% 
  mutate(quarter = as.yearqtr(date)) %>%
  group_by(quarter) %>%
  summarize(value = mean(value, na.rm = TRUE)) %>%
  ungroup()

# Filter the series to start from the first quarter of 1980 and end in the last quarter of 2019
unemp_rate <- series_quarterly %>%
  filter(quarter >= as.yearqtr("1980 Q1") & quarter <= as.yearqtr("2019 Q4"))


# Set up the plotting area
par(mfrow=c(1, 2))

# Plot ACF with lags limited to 10
acf(unemp_rate$value, main = "ACF for Unemployment Rate", lag.max = 10)

# Plot PACF with lags limited to 10
pacf(unemp_rate$value, main = "PACF for Unemployment Rate", lag.max = 10)

# Reset the plotting area back to default
par(mfrow=c(1, 1))

# Conduct the ADF test to check for stationarity
adf_test_result <- ur.df(unemp_rate$value, type = "drift", lags = 4, selectlags = "AIC")

```

@fig-unemp above explains that a lag 4 should be used for the augmented DF test. Note that this test (see Appendix) suggests that the series is not stationary at the 95% confidence level. We therefore use the differenced series.

The fourth series, the USD Index, is also from the Reserve Bank of Australia (RBA) exchange rates statistical tables [@RBA2024d]. This is  a monthly series. The series was converted into a quarterly series by choosing the relevant months for the beginning of each quarter, starting from March and ending with December.

```{r USD Index}
#| echo: false
#| message: false
#| warning: false
#| label: fig-Index
#| fig-cap: "ACF and PACF functions for the USD Trade Weighted Index"

# Downloading USD Trade Weighted Index data from the internet (March 1973 = 100)

# Reading and preparing the USD TWI data
series <- read_rba(series_id = "FUSXRTWI") 

#clean the series by removing missing values
series <- na.omit(series)

# Convert the date format from dd/mm/yyyy to Date object
series <- series %>% 
  mutate(date = as.Date(as.character(date), format = "%Y-%m-%d")) 

# Ensuring that date is recognized as a quarterly object and summarizing to quarterly values
series_quarterly <- series %>% 
  mutate(quarter = as.yearqtr(date)) %>%
  group_by(quarter) %>%
  summarize(value = mean(value, na.rm = TRUE)) %>%
  ungroup()

# Filter the series to start from the first quarter of 1980 and end in the last quarter of 2019
USD_Index <- series_quarterly %>%
  filter(quarter >= as.yearqtr("1980 Q1") & quarter <= as.yearqtr("2019 Q4"))

# Apply logarithmic transformation to the USD Index values
USD_Index$value <- log(USD_Index$value)

# Set up the plotting area
par(mfrow=c(1, 2))

# Plot ACF with lags limited to 10
acf(unemp_rate$value, main = "ACF for USD Index", lag.max = 10)

# Plot PACF with lags limited to 10
pacf(unemp_rate$value, main = "PACF for USD Index", lag.max = 10)

# Reset the plotting area back to default
par(mfrow=c(1, 1))

# Augmented Dickey-Fuller test to check for stationarity
adf_test_result <- ur.df(USD_Index$value, type = "drift", lags = 4, selectlags = "AIC")

```

@fig-Index above indicates that a lag 4 should be used for the augmented DF test. The results suggest that the series is not stationary at the 95% confidence level. We therefore use the differenced series.

### Augmented Dicky-Fuller test results for the USD Index

```{r Calculating the ADF test for each series} 

# Perform ADF tests for each series
adf_results <- list(
  GDP = ur.df(GDP_rate$value, type = "drift", lags = 4),
  CPI = ur.df(CPI_rate$value, type = "drift", lags = 4),
  Unemployment = ur.df(unemp_rate$value, type = "drift", lags = 4),
  USD_Index = ur.df(USD_Index$value, type = "drift", lags = 4)
)

# Extracting test statistics and critical values
adf_summary <- sapply(adf_results, function(result) {
  c(
    Test_Statistic = result@teststat[1],  # Extract the test statistic
    Critical_Value_5pct = result@cval[1, "5pct"]  # Extract the 5% critical value
  )
})

# Convert the summary into a data frame for better formatting
adf_summary_df <- as.data.frame(t(adf_summary))
adf_summary_df$Variable <- rownames(adf_summary_df)

# Rearrange columns
adf_summary_df <- adf_summary_df[, c("Variable", "Test_Statistic", "Critical_Value_5pct")]

# Display the table
kable(adf_summary_df, caption = "ADF Test Results Summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))


```

### Do we need to use stationary data series

After consultations with Dr. Thomaz Wozniak, my understanding is that it is possible to use unit-root nonstationary variables in dynamic modeling. Based, on this, all rates are used without differencing, and the USD Index is used in logged form, also without differencing.

```{r plotting all four series}
#| echo: false
#| message: false
#| warning: false
#| label: fig-combined
#| fig-cap: "Treated series plots"


p1 <- ggplot(GDP_rate, aes(x = date, y = value)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Quarterly GDP Rate (1980-2019)", x = "Quarter", y = "GDP Growth Rate (%)") +
  theme(plot.title = element_text(size = 9))

datasets <- list(CPI_rate = CPI_rate, unemp_rate = unemp_rate, USD_Index = USD_Index)
titles <- c("Quarterly CPI Rate (1980-2019)",
            "Quarterly Unemployment Rate (1980-2019)", "Log of Quarterly USD Index (1980-2019)")
y_titles <- c("CPI Rate (%)","Unemployment Rate (%)", 
              "Log(USD Index)")

# Empty list to store the plots
plot_list <- list()

# Loop over the datasets and titles
for (i in seq_along(datasets)) {
  # Create the plot
  plot <- ggplot(datasets[[i]], aes(x = quarter, y = value)) +
    geom_line() +
    theme_minimal() +
    labs(title = titles[i], x = "Quarter", y = y_titles[i]) +
    theme(plot.title = element_text(size = 9))
  
  # Add the plot to the list
  plot_list[[i]] <- plot
}

plot_list <- c(list(p1), plot_list)
grid.arrange(grobs = plot_list, ncol = 2)


```

### Model Specification


The above selection and treatment of the data is pivotal to answering the research question: How does a weakening United States Dollar (USD) influence overall economic stability in Australia? By focusing on key economic indicators like the GDP growth rate, unemployment rate, and inflation rate, and comparing these against the USD Trade Weighted Index (TWI), the study strategically addresses the multifaceted impact of USD fluctuations on the Australian economy. The chosen data encapsulate the essential aspects of economic stability, allowing for a nuanced analysis of the interdependencies between the USD's value and Australian economic health.

This approach is vital for constructing a comprehensive Bayesian Structural Vector Autoregression (SVAR) model. The SVAR model, designed to elucidate the transmission mechanisms of USD fluctuations, relies on accurate, time-aligned, and contextually relevant data to provide meaningful insights. By ensuring the data are stationary and appropriately preprocessed for the chosen analysis timeframe (1980-2019), excluding periods of atypical economic disruption like the COVID-19 crisis, the document ensures that the model's outputs are reflective of standard economic interactions, enhancing the reliability of the findings.

Furthermore, the data treatment, including the conversion of series to quarterly frequencies and addressing stationarity, directly informs the model equations by ensuring that the inputs reflect genuine economic trends devoid of seasonal or non-stationary noise. This data preparation not only strengthens the study’s methodological framework but also ensures that the SVAR model's findings will offer actionable insights, enabling policymakers and analysts to base decisions on solid empirical evidence, thereby underlining the importance of investigating the specified problem.

Next, we employ a Bayesian Structural Vector Autoregression (SVAR) model to understand the dynamic relationships among various economic indicators influenced by fluctuations in the United States Dollar (USD) against the Australian Dollar (AUD) [@wozniakBsvarsBayesianEstimation2022]. In particular, we follow @Bhuiyan2012 who introduces a Bayesian structural VAR model tailored for Canada, focusing on evaluating the impact of monetary policy shocks, utilizing the overnight rate target as the main policy tool. However, our model differs in that it does not directly account for any fiscal and monetary policy,  rather it evaluates the potential of using the US Index to predict the stability of the Australian economy. 

#### Justification for the Selection of the Bayesian SVAR Model

In addressing the dynamics between the United States Dollar (USD) and the Australian economy, we opt for a Bayesian Structural Vector Autoregression (SVAR) model over alternative econometric methodologies for several reasons. Firstly, the SVAR model allows us to incorporate prior knowledge and uncertainties into our analysis, an advantage not readily available in traditional VAR models. This Bayesian approach is particularly beneficial in economic studies where historical data and expert judgment play crucial roles in shaping analysis and expectations.

Furthermore, SVAR models enable us to decipher the structural impacts of economic shocks, such as fluctuations in the USD, on various indicators of economic stability within Australia, including GDP growth, inflation, and unemployment rates. Unlike standard VAR models that treat all innovations as endogenously generated within the system, SVAR models allow us to impose structural restrictions based on economic theory. This is critical for understanding the directionality and magnitude of relationships between variables, thereby offering a more nuanced analysis of economic interdependencies.

#### Interpretation of the $B_0$ Matrix in the Bayesian SVAR Model

In our Bayesian SVAR model, the matrix $B_0$ is pivotal as it encapsulates the contemporaneous relationships between the endogenous variables. Each entry $b_{ij}$ in the $B_0$ matrix can be interpreted as the immediate impact of a one-unit shock in variable $j$ on variable $i$ within the same time period, holding all other factors constant. This interpretation allows us to understand the intricate web of interactions within the economic system, such as how a sudden change in the USD Index might immediately affect Australia's GDP growth, inflation, or unemployment rate.

For instance, a positive entry in the row of the GDP growth and in the column of the USD Index would indicate that an instantaneous increase in the USD Index (reflecting a strengthening of the USD) has a positive impact on Australian GDP growth within the same period. Such insights are invaluable for policymakers and analysts seeking to anticipate and mitigate the effects of international monetary fluctuations on domestic economic stability.

This is a 4x1 vector of endogenous variables at time t, where:

- $USD\_Index$ : Log of the Trade Weighted Index of the US Dollar.
- $Unemployment\_rate$ : unemployment rate.
- $Inflation\_rate$ : inflation rate.
- $GDP\_growth$ : growth rate of GDP.


The SVAR model structure:

- $B_0$ is a 4X4 matrix representing the contemporaneous relationships between the variables.
- $B_i$ (for i = 0,1,...,p) are 4x4 matrices representing the lagged effects (where p is the lag order).
- $u_t$ is a vector of innovations at time $t$, assumed to follow $u_t \mid Y_{t-1} \sim N(0, I_N)$, where $I_N$ is the identity matrix. This implies that shocks are independently and identically distributed with no correlation and a standard normal distribution.


$$ 
Y_t =
\begin{bmatrix}
    USD\_Index_t \\
    Inflation\_rate_t \\
    Unemployment\_rate_t\\
    GDP\_growth_t
\end{bmatrix}
$$

The Bayesian Structural VAR model can then be written as: 

The Bayesian Structural Vector Autoregression (SVAR) model is given by the equation:

$$
B_0 Y_t = b_0 + B_1 Y_{t-1} + \dots + B_p Y_{t-p} + u_t
$$


#### Model Notations:

$Y_t$: This represents the vector of endogenous variables at time tt. In this context, the vector consists of the Trade Weighted Index of the US Dollar (USD_Index), the unemployment rate, the inflation rate, and the GDP growth rate. These variables are chosen because they represent key indicators of economic stability.

$B_0$: This is a matrix representing the contemporaneous relationships between the variables in $Y_t$. In an SVAR model, this matrix helps to understand how shocks to one variable (e.g., USD_Index) can contemporaneously affect other variables (e.g., unemployment, inflation, GDP growth).

$B_i$ (for i=0,1,...,pi=0,1,...,p): These are matrices representing the lagged effects of the variables on each other. The subscript ii represents the lag order, showing how past values (lags) of each variable in $Y_t$ influence the current values. In time series analysis, this captures dynamics such as how past economic conditions influence current outcomes.

$u_t$: This is the vector of errors or shocks at time tt. In the SVAR framework, these are considered unobservable random shocks that affect the endogenous variables. In Bayesian SVAR models, these shocks are also subjected to prior distributions reflecting our beliefs or assumptions before observing the data.

#### Using the Model to Answer the Research Question:

The model will be used to analyze how fluctuations in the US Trade Weighted Index (representing a weakening or strengthening USD) impact the Australian economy's stability, specifically through GDP growth, inflation, and unemployment rates. By examining the contemporaneous and lagged relationships between these variables, we can understand the transmission mechanisms of US dollar fluctuations.

To address the research objectives, we will assess the dynamic effects of the USD_Index on Australia's economic indicators. This involves looking at the impulse response functions (IRFs) and variance decompositions derived from the SVAR model. The IRFs will show how a shock to the US dollar index impacts Australian GDP growth, inflation, and unemployment over time. This directly speaks to the study's aim to elucidate the transmission mechanisms of such fluctuations.

This analysis will be conducted using a stepwise approach:

- Lag Selection: Optimal lags for the variables will be determined based on the general approach in the literature for quarterly data, namely, 12 lags.

- Model Estimation: The extended SVAR model, incorporating the selected lags, will be estimated to analyze the interactions between the USD Index and Australian economic indicators.

#### Estimation Outputs for Interpreting the Research Question:

**Posterior Distributions**: These reflect the updated beliefs about the model's parameters after considering the data. We would examine the posterior distributions of the SVAR model parameters, particularly focusing on the coefficients that measure the impact of USD_Index changes on the economic indicators.

**Impulse Response Functions (IRFs)**: IRFs trace the effects of a one-time shock to one of the innovations on the current and future values of the endogenous variables. By analyzing the IRFs, we can interpret how an unexpected change in the US dollar's value affects Australian economic conditions over time.

**Variance Decompositions**: These help us understand the proportion of the forecast variance of each endogenous variable that can be attributed to shocks to each variable in the model, including shocks to the USD_Index. This is crucial for assessing the significance of US dollar fluctuations compared to other domestic factors.

**Credibility Intervals**: Unlike classical confidence intervals, credibility intervals in Bayesian analysis offer a probability statement about the parameter values. Examining these intervals for the effects of interest will provide insights into the certainty of our estimates.

In terms of economic context, understanding the relationship between the US dollar index and Australian economic indicators is vital for policymakers and economic analysts, especially considering the significant trade and financial ties between the two countries. The model helps address the research objectives by providing a structured way to quantify these relationships, thereby informing decisions that could enhance economic stability in response to international currency movements.

For your Bayesian SVAR model examining the impact of the U.S. Dollar on the Australian economy, applying exclusionary restrictions to the $B_0$ matrix can be guided by economic theory concerning the likely immediate (contemporaneous) effects of one variable on another. In the context of your study, focusing on GDP growth, inflation, and unemployment as responses to changes in the USD Trade Weighted Index, here are some suggestions for structuring these restrictions:

#### Considerations for $B_0$ Matrix Exclusionary Restrictions

Using a lower triangular matrix for $B_0$ in our Bayesian Structural Vector Autoregression (SVAR) model is a common approach in identifying the model. This approach can help ensure that the model is identified under the assumption that the contemporaneous relationships between the variables follow a specific causal ordering. Here's how we  implement and justify this approach along with leveraging the Minnesota prior for testing and imposing restrictions:

#### Using a Lower Triangular Matrix for $B_0$

##### Advantages
- **Causality Assumption**: A lower triangular matrix implies that a variable does not contemporaneously affect any variables that come before it in the ordering. This reflects a causal hierarchy among the variables, where earlier variables can affect later ones within the same time period, but not vice versa.
- **Model Identification**: SVAR models often suffer from identification issues due to the symmetric nature of the covariance matrix of the errors. Using a lower triangular $B_0$ can help in uniquely determining the model by providing a clear causal path among variables.

##### How to Implement
- **Ordering of Variables**: Decide the ordering of variables based on economic theory or empirical evidence. For our analysis on the effects of a weakening USD:
  1. **USD Index**: As the exogenous shock.
  2. **Inflation Rate**: Likely responds quickly to currency value changes due to import prices.
  3. **Unemployment Rate**: Might react to changes in economic conditions that are influenced by inflation adjustments.
  4. **GDP Growth**: Often considered to respond over a longer period to changes in broader economic indicators.
- **Matrix Structure**: With this ordering, our $B_0$ matrix would look like:

  $B_0 =$ 
  \begin{bmatrix}
    b_{11} & 0 & 0 & 0 \\
    b_{21} & b_{22} & 0 & 0 \\
    b_{31} & b_{32} & b_{33} & 0 \\
    b_{41} & b_{42} & b_{43} & b_{44}
  \end{bmatrix}

  Here, $b_{ij}$ are parameters to be estimated, with non-zero entries allowed only in the lower triangle, reflecting the immediate impacts as per the assumed causality.

#### Testing Restrictions Using the Minnesota Prior

##### Minnesota Prior and Shrinkage
- **Role of Minnesota Prior**: In Bayesian SVARs, the Minnesota prior typically imposes shrinkage on the parameters, pulling them towards some baseline values (often zero or a random walk hypothesis for autoregressive parameters). This can be particularly useful in imposing and testing exclusion restrictions implicitly through the prior distribution.
- **Implementing Shrinkage**: We use the Minnesota prior to differentially shrink the off-diagonal elements of $B_0$ to test whether the data supports the exclusion restrictions. Larger shrinkage (higher prior variance) on certain parameters can test if the unrestricted model significantly deviates from these baseline assumptions.

##### Steps to Follow
1. **Specify the Prior**: Define the Minnesota prior for each element in $B_0$ based on how strongly our believe in the causal ordering and immediate impacts.
2. **Fit the Model**: Use Bayesian estimation techniques. We will use the Gibbs sampling proposed by @Waggoner&Zha2003 to fit the SVAR model.
3. **Analyze the Posterior**: Check the posterior distributions of the $B_0$ elements. If the posterior distributions of certain off-diagonal elements concentrate around zero (or within a close range), it supports the exclusion restrictions under the causal ordering.
4. **Robustness Checks**: Consider alternative orderings or less restrictive priors to ensure robustness of the findings.

This approach not only aids in model identification and ensures adherence to theoretical expectations but also leverages the flexibility of Bayesian methods to incorporate and test economic theory effectively.

### Derivation of the Estimation Algorithm 

For the basic model as described above, we will proceed as follows: 

#### Model Structure

The SVAR model is expressed as:
$B_0 Y_t = B_+ X_t + u_t$
where:
- $B_0$ contains contemporaneous effects with certain zero restrictions (exclusion restrictions).
- $B_+$ is the matrix comprising the intercept and coefficients of lagged variables.
- $X_t$includes a constant and lagged values of \( Y \).
- $u_t$ is the error term assumed to be normally distributed.

#### Exclusion Restrictions
We have specified that each row $n$ of $B_0$ can be represented as a product of unrestricted coefficients $b_n$ and a fixed matrix $V_n$ of zeros and ones, applying structural constraints to the model.

#### Likelihood Function
The likelihood function for the model incorporates the determinants and exponentials of quadratic forms, which reflect the usual Gaussian assumptions for the innovations.

The likelihood function usning the Normal-Generalized-Normal (NGN) distribution is detailed below: 

$$
\mathcal{L}(B_+, B_0 \mid Y, X) = \left| \det (B_0) \right|^T \exp \left( -\frac{1}{2} \sum_{n=1}^N \left( b_n V_n Y - B_n X \right) \left( b_n V_n Y - B_n X \right)' \right)
$$

Expanded Expression

Expanding the expression within the exponential term:

$$
\sum_{n=1}^N \left( b_n V_n Y Y' V_n' b_n' - 2 b_n V_n Y X' B_n' + B_n X X' B_n' \right)
$$

And further: 

$$
\begin{aligned}
    b_n V_n Y Y' V_n' b_n' - 2 b_n V_n Y X' B_n' + B_n X X' B_n' &= b_n V_n \left( Y Y' - Y X' (X X')^{-1} X Y' \right) V_n' b_n' \\
    &\quad + (B_n - b_n V_n \hat{A}) (X X') (B_n - b_n V_n \hat{A})'
\end{aligned}
$$

where $\hat{A} = YX' (XX')^{-1}$.

Likelihood Function Simplification

The simplified likelihood function can then be presented as:

$$
\mathcal{L}(B_+, B_0 \mid Y, X) \sim \mathcal{NGN} \left( \hat{A}, (X X')^{-1}, (Y Y' - Y X' (X X')^{-1} X Y')^{-1}, T + N \right)
$$



#### Prior and Posterior Distributions
The model uses a Normal-generalized-normal (NGN) distribution approach for the priors. The likelihood and priors are combined to form the posterior distributions, which are sampled using Gibbs sampling.

Natural-Conjugate Prior

The NGN distribution as a natural-conjugate prior:

$$
p(B_+, B_0) \sim \mathcal{NGN} \left( B, \Omega, S, \nu \right)
$$

$$p(B_+, B_0) = \prod_{n=1}^N p(B_n \mid b_n) \cdot p(b_1, \ldots, b_N)
$$

$$p(B_n \mid b_n) \sim \mathcal{N}_K \left( b_n V_n B, \Omega \right)
$$

$$p(b_1, \ldots, b_N) \propto \left| \det(B_0) \right|^{\nu-N} \exp \left( -\frac{1}{2} \sum_{n=1}^N b_n V_n S^{-1} V_n' b_n' \right)
$$

Therefore, 

$$
\begin{aligned}
p(B_+; B_0) &= \left| \det(B_0) \right|^{\underline{\nu}-N} \exp \left\{ -\frac{1}{2} \sum_{n=1}^{N} b_n V_n \underline{S}^{-1} V_n^T b_n^T \right\} \\
&\quad \times \exp \left\{ -\frac{1}{2} \sum_{n=1}^{N} (B_n - b_n V_n \underline{B}) \ \ \underline{\Omega}^{-1} \ \ (B_n - b_n V_n \underline{B})^T \right\}
\end{aligned}
$$

Posterior Distribution

Combining the likelihood and the prior, the posterior distribution:

$$
p(B_+, B_0 \mid Y, X) \propto \mathcal{L}(Y \mid B_+, B_0, X) \cdot p(B_+, B_0)
$$

$$
\begin{align*}
= \left| \det (B_0) \right|^{T+\nu-N} \exp \Bigg( -\frac{1}{2} \sum_{n=1}^N \Bigg( & b_n V_n Y Y' V_n' b_n' - 2 b_n V_n Y X' B_n' + B_n X X' B_n' \\
& + b_n V_n S^{-1} V_n' b_n' + B_n \Omega^{-1} B_n' \\
& - 2 b_n V_n B \Omega^{-1} B_n' + b_n V_n B \Omega^{-1} B' V_n' b_n'
\Bigg) \Bigg)
\end{align*}
$$

The resulting posterior parameters are as follow: 

$$
\bar{\Omega} = [XX^T+\underline{{\Omega}} \ ]^{-1}
$$


$$
\bar{B} = [YX^T+\underline{B{\Omega}}^{-1}]\bar{\Omega}
$$

$$
\bar{S} = [YY^T+\underline{S}^{-1}+ \underline{B{\Omega}}^{-1} \underline{B}^T - \overline{B{\Omega}}^{-1} \bar{B}]^{-1}
$$
$$
\bar{\nu} = T + \underline{\nu} 
$$

Using these formulas we can proceed to sample using the Gibbs sampling procedure. 

#### Gibbs Sampling Procedure
Gibbs sampling for this model involves:
1. **Sampling the unrestricted coefficients $b_n$** from their conditional distributions given the data and all other parameters.
2. **Sampling $B_+$**, considering the latest draws of $b_n$ and the data.

#### Steps to Implement in R

To estimate this model in R, you would generally need to:

1. **Define the Model Structure:**
   - We set up matrices $B_0$ and $B_+$ based on the given structure and exclusion restrictions.
   - We prepare the data matrices $Y$ and $X$.

2. **Initialize Parameters:**
   - We initialize $B_0$, $B_+$, and any other matrices or parameters required for the Gibbs sampling.

3. **Implement Gibbs Sampler:**
   - We write loops to update each parameter iteratively based on its conditional posterior distribution. This includes:
     - Computing residuals and other derived quantities needed to sample $b_n$ and $B_+$.
     - Drawing from the conditional distributions using function  `rmvnorm` for multivariate normal distributions.

4. **Normalize and Summarize Results:**
   - After obtaining the draws, we normalize and summarize the posterior distributions to get estimates and credible intervals for the parameters.


### Model Testing Using A Random Walk 

We use a random walk to generate a $Y$ matrix with one lag, and with 300 observations over two variables, to check whether the estimation algorithm is able to reproduce the random walk structure. In other words, we expect that $B_0$ will equal an identity matrix, the intercept will equal zero, and the remaining part of the $B_+$ matrix will also be an identity matrix. 

We used 100 iterations for the burn-in phase to allow the algorithm to converge, then used 500 iteration for the estimation.

```{r creating Y and X matrices using a random walk, echo=FALSE, eval=TRUE}

set.seed(1234567)

# Function to simulate a multivariate Gaussian random walk
simulate_random_walk <- function(n, initial_value, sigma) {
  K <- length(initial_value)
  data <- matrix(nrow = n, ncol = K, data = NA)
  data[1,] <- initial_value
  for (i in 2:n) {
    data[i,] <- data[i-1,] + mvrnorm(1, mu = rep(0, K), Sigma = sigma)
  }
  return(data)
}

# Simulate the Y matrix as a random walk
n <- 100  # Number of observations
initial_values <- rep(0, 4)  # Initial values for each of the 2 variables
sigma <- diag(0.1, 4)  # Variance for each variable in the random walk
Y <- simulate_random_walk(n, initial_values, sigma)

```


```{r defining the Vn matrices for the exclusion restrictions on B0 for the basic model} 

# Create a list V with predefined fixed matrices
create_V_matrices <- function() {
  
  V <- list()
  
  V[[1]] <- matrix(c(1, 0, 0, 0), nrow = 1, ncol = 4, byrow = TRUE)
  V[[2]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0), nrow = 2, ncol = 4, byrow = TRUE)
  V[[3]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0), nrow = 3, ncol = 4, byrow = TRUE)
  V[[4]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0,
                     0, 0, 0, 1), nrow = 4, ncol = 4, byrow = TRUE)
  
  return(V)
}

# Generate the list of V matrices
V_matrices <- create_V_matrices()

```

We used 100 iterations for the burn-in phase to allow the algorithm to converge, then used 500 iteration for the estimation.

```{r estimating B0 and Bp for the random walk, echo=FALSE, eval=TRUE}

# Call the function
posterior_results <- generate_posteriors(Y, V_matrices, kappa1 = 0.0001, kappa2 = 0.01, kappa3 =  0.01, kappa4 = 1)

# The function returns a list containing B0 and Bp posteriors which can be accessed as follows:
B0_posterior0 <- posterior_results$B0
Bp_posterior0 <- posterior_results$Bp


```

The estimated $B_0$ and $B_+$ are shown below. 

```{r Ranom Walk  B0 & Bp matrix, echo=FALSE, eval=TRUE}

print("Random Walk Testing Normalized B0 matrix")
print(B0_posterior0 [,,S])

print("Random Walk Testing Bp matrix")
print(Bp_posterior0 [,,S])

```

Using only one lag for illustrative purposes, there is a clear convergence of the B0 and Bp matrix towards an identity matrix. The other elements in this matrix and all elements in Bp are also approaching zero (because we opted for no intercept). We have enough evidence to see that the algorithm is able to estimate the random walk process.

### Estimation of the Basic Model

We now repeat the same steps above to estimate the $B_0$ and $B_+$ matrices for the basic model. 

```{r estimating the Basic Bayesian SVAR model, echo=FALSE, eval=TRUE}

results <- estimate_basic_model(S_burnin, S)

B0_posterior1 <- results$B0_posterior
Bp_posterior1 <- results$Bp_posterior

```

```{r printing B0 & Bp for the Basic model, echo=FALSE, eval=TRUE}

print("Basic Model Normalized B0 matrix")
print(B0_posterior1[ , , S])

print("Basic Model B plus matrix")
print(Bp_posterior1[ , ,S])

```


```{r calculating the standard deviation for the Basic Model B0 & Bp matrices, echo=FALSE, eval=TRUE}

calculate_and_print_B0sd(B0_posterior1)
  
calculate_and_print_Bpsd(Bp_posterior1) 

```

```{r lagged Bp plots for the Basic model, echo=FALSE, eval=TRUE}

plot_lagged_effects(B0_posterior1, Bp_posterior1)

```

```{r calculate the significance levels for the Basic Model B0 and Bp, echo=FALSE, eval=TRUE}

# Calculate significance for B0 and Bp matrices
B0_significance <- calculate_significance(B0_posterior1)
Bp_significance <- calculate_significance(Bp_posterior1)

# Print significance matrices
print("Significance for B0 Matrix:")
print(B0_significance)
print("Significance for Bp Matrix:")
print(Bp_significance)


```

```{r Plotting the IRF response for the basic model, echo=FALSE, eval=TRUE}

B0 <- B0_posterior1
Bp <- Bp_posterior1

plot_irfs(B0, Bp, horizon = 10, conf_level = 0.95)

```


### Model Extension I: Solving the Identification Problem Using a Non-Normality Approach

We present the Bayesian estimation and inference methodology for an N-variate Student-t distribution modeled as an IG2-scale mixture of normals. The derivation of the full conditional posterior distributions is expanded step-by-step, and the Metropolis-Hastings algorithm is used for sampling the degrees of freedom parameter $\nu$. An R function implementing the sampler is provided.

The Student-t distribution is commonly used in statistical modeling to handle data with heavier tails than the normal distribution. An essential parameter of the Student-t distribution is the degrees of freedom $\nu$, which controls the tail heaviness. In this note, we present the Bayesian estimation of the degrees of freedom parameter for an N-variate Student-t distribution using the Inverted-Gamma 2 (IG2) scale mixture of normals.

The N-variate Student-t distribution can be represented as a scale mixture of normals:

# Model Setup

The model is given by:
$$
B_0 Y = B_p X + U_t, \quad U_t \sim \mathcal{N}(0_T, \lambda I_T)
$$

The priors are:
$$
p(B_0, B_p) \sim \mathcal{NGN}(\underline{B}, \underline{\Omega}, \underline{S}, \underline{\nu})
$$
$$
\lambda \sim \mathcal{IG2}(\underline{s}, \underline{\nu})
$$

Where $\underline{s} = \underline{\nu} - 2$ to ensure that the mean is eaual to one, as was the case under the basic model. 

# Likelihood Function

The likelihood function for the model is:
$$
p(Y \mid B_0, B_p, \lambda, X) = (2\pi\lambda)^{-\frac{T}{2}} \exp\left( -\frac{1}{2\lambda} \sum_{t=1}^T (B_0 Y_t - B_p X_t)^T (B_0 Y_t - B_p X_t) \right)
$$

# Prior for $\lambda$

The Inverted-Gamma 2 prior for $\lambda$ is:
$$
p(\lambda) = \frac{(s/2)^{\nu/2}}{\Gamma(\nu/2)} \lambda^{-\nu/2-1} \exp\left( -\frac{s}{2\lambda} \right)
$$

# Posterior for $\lambda$

Combining the likelihood and the prior for $\lambda$, we get:
$$
p(\lambda \mid Y, B_0, B_p, X) \propto \lambda^{-\left( \frac{T}{2} + \frac{\nu}{2} + 1 \right)} \exp\left( -\frac{1}{2\lambda} \left( \sum_{t=1}^T (B_0 Y_t - B_p X_t)^T (B_0 Y_t - B_p X_t) + s \right) \right)
$$

This is the kernel of an Inverted-Gamma 2 distribution:
$$
\lambda \mid Y, B_0, B_p, X \sim \mathcal{IG2}\left( \nu + T, \sum_{t=1}^T (B_0 Y_t - B_p X_t)^T (B_0 Y_t - B_p X_t) + s \right)
$$

The posterior mean of $\lambda$ is:
$$
\hat{\lambda} = \frac{s + \sum_{t=1}^T (B_0 Y_t - B_p X_t)^T (B_0 Y_t - B_p X_t)}{\nu + T - 2}
$$


### Combining with the Prior

Assuming the prior $p(B^+; B_0)$ as given or formulated according to specific assumptions (such as independence, Gaussian priors for coefficients, etc.), the full posterior kernel combining this likelihood with the prior can be approached as:

$$
p(B_+; B_0 \mid Y; X) \propto p(Y \mid X, B_+, B_0) \times p(B_+; B_0)
$$

This expression must often be evaluated or sampled using computational Bayesian techniques, such as Markov Chain Monte Carlo (MCMC), especially given the integrative step required for $\lambda$.

We can obtain the following posterior formulas similar to the basic model. Note that we use $\underline{\Omega} = \underline{\lambda\Omega}$ in the following equations: 

$$
\bar{\Omega} = [XX^T+ \underline{{\Omega}} \ ]^{-1}
$$

$$
\bar{B} = [YX^T+\underline{B{\Omega}}^{-1}]\bar{\Omega}
$$

$$
\bar{S} = [YY^T+\underline{S}^{-1}+ \underline{B{\Omega}}^{-1} \underline{B}^T - \overline{B{\Omega}}^{-1} \bar{B}]^{-1}
$$

Where 

$$
\underline{S} = ( s + \sum_{t=1}^T (B_0 Y_t - B_p X_t)^T (B_0 Y_t - B_p X_t)) * I_N
$$ 


## Posterior for $\bar{\nu}$

$$
\bar{\nu} = T + \underline{\nu}
$$

Where $\underline{\nu} = \underline {s} + 2 = 5 + 2 = 7$. 


# Conclusion

The parameters $\alpha$ and $\beta$ from the prior distribution of $\lambda$ affect the posterior distributions for $B_0$ and $B_p$ through the variance scaling in the likelihood, influencing $\bar{S}$. This approach correctly shows that $\lambda$ does not explicitly appear in the posterior formulas for $\bar{\Omega}$ and $\bar{B}$, but $\lambda$ impacts the overall posterior distribution implicitly. This rigorous derivation demonstrates the correct Bayesian inference for a hierarchical model, incorporating the effects of $\lambda$ through its posterior distribution.


```{r calculate B0 and B+ for the Extension I Bayesian SVAR model, echo=FALSE, eval=TRUE}

estimate_extension_I_model <- function (S_burnin, S){


  
  matrices <- create_matrices()
  Y = matrices$Y

  y   = Y
   
  # Basic settings and model dimensions
  N <- ncol(y)
  p <- 12   # This is the number of lags in the model
  K <- 1 + N * p 

# create Y and X
############################################################
Y       = y[(p+1):nrow(y),]
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[((p+1):nrow(y))-i,])
}

Y       = t(Y)
X       = t(X)


 V <- list()
 
  V[[1]] <- matrix(c(1, 0, 0, 0), nrow = 1, ncol = 4, byrow = TRUE)
  V[[2]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0), nrow = 2, ncol = 4, byrow = TRUE)
  V[[3]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0), nrow = 3, ncol = 4, byrow = TRUE)
  V[[4]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0,
                     0, 0, 0, 1), nrow = 4, ncol = 4, byrow = TRUE)

# Set the priors
kappa1 <- 0.01      # autoregressive slope shrinkage
kappa2 <- 1         # constant term shrinkage 
kappa3 <- 1000      # contemporaneous effects shrinkage
kappa4 <- 1         # VAR prior persistence

# Basic settings and model dimensions
N <- ncol(y)
p <- 12   # This is the number of lags in the model
K <- 1 + N * p

priors <- list(
  B = cbind(rep(0, N), kappa4 * diag(N), matrix(0, N, (p - 1) * N)),
  Omega = diag(c(kappa2, kappa1 * ((1:p)^(-2)) %x% rep(1, N))),
  S = kappa3 * diag(N),
  nu = N
)

# Initialize B0
B0.initial <- matrix(0, N, N)
for (n in 1:N) {
  unrestricted <- apply(V[[n]], 2, sum) == 1
  B0.initial[n, unrestricted] <- rnorm(sum(unrestricted))
}

N = nrow(B0.initial)
no.draws = S_burnin

B0 = array(NA, c(N, N, no.draws))
Bp = array(NA, c(nrow(priors$B), ncol(priors$B), no.draws))

B0.aux = B0.initial
Bp.aux = matrix(NA, nrow(priors$B), ncol(priors$B))

# Select the first value for lambda 
shape <- 7
scale <- 5

for (i in 1:S_burnin) {
  
  # Set the priors
  kappa1 <- 0.01      # autoregressive slope shrinkage
  kappa2 <- 1         # constant term shrinkage 
  kappa3 <- scale     # contemporaneous effects shrinkage
  kappa4 <- 1         # VAR prior persistence

  priors <- list(
  B = cbind(rep(0, N), kappa4 * diag(N), matrix(0, N, (p - 1) * N)),
  Omega = diag(c(kappa2, kappa1 * ((1:p)^(-2)) %x% rep(1, N))),
  S = kappa3 * diag(N),
  nu = shape
)
  
  # Compute posterior distribution parameters
  Omega.inv <- solve(priors$Omega)
  Omega.post.inv <- (X %*% t(X) + Omega.inv)
  Omega.post <- solve(Omega.post.inv)
  B.post <- (Y %*% t(X) + priors$B %*% Omega.inv) %*% Omega.post
  S.post <- solve(Y %*% t(Y) + solve(priors$S) + priors$B %*% Omega.inv %*% t(priors$B) - B.post %*% Omega.post.inv %*% t(B.post) +  matrix(scale^(-1), nrow = N, ncol = N))
  nu.post <- nrow(y) + priors$nu  
  
  # Check if Omega.post and S.post is positive definite and store the inverse if it is
  is_positive_definite <- tryCatch({
    chol(Omega.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  is_positive_definite <- tryCatch({
    chol(S.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }
  
  S.inv <- solve(S.post)
  nu = nu.post
  

  for (n in 1:nrow(B0.initial)) {
    rn = nrow(V[[n]])
    Un = chol(nu * solve(V[[n]] %*% S.inv %*% t(V[[n]])))
    w = t(orthogonal.complement.matrix.TW(t(B0.aux[-n, ])))
    w1 = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
    if (rn > 1) {
      Wn = cbind(t(w1), orthogonal.complement.matrix.TW(t(w1)))
    } else {
      Wn = w1
    }
    alpha = rep(NA, rn)
    u = rmvnorm(1, rep(0, nu + 1), (1 / nu) * diag(nu + 1))
    alpha[1] = sqrt(as.numeric(u %*% t(u)))
    if (runif(1) < 0.5) {
      alpha[1] = -alpha[1]
    }
    if (rn > 1) {
      alpha[2:rn] = rmvnorm(1, rep(0, nrow(V[[n]]) - 1), (1 / nu) * diag(rn - 1))
    }
    bn = alpha %*% Wn %*% Un
    B0.aux[n, ] = bn %*% V[[n]]
    
    K = ncol(B.post)
    L = t(chol(Omega.post))
    
    Bp.aux[n, ] = as.vector(t(B0.aux[n, ] %*% B.post) + L %*% rnorm(K))
  }
  
  B0[, , i] = B0.aux
  Bp[, , i] = Bp.aux


  total       = 0 
  
  for (j in 1:ncol(Y)) {
    
    A = Y[, j]
    A = matrix(A, ncol = 1)
    B = X[, j]
    B = matrix(B, ncol = 1)
    C = (B0.aux %*% A - Bp.aux %*% B)
    
    total = total + t(C) %*% C
    
  }
  

    scale = total          # This is the scale parameter
    scale = scale[1, 1]    # Converting scale into a scalar
  
}

B0_posterior <- B0

# Initialize B0 again for the remainder of the iterations 
B0.initial <- B0_posterior[, , S_burnin]
 
N = nrow(B0.initial)
no.draws = S

B0 = array(NA, c(N, N, no.draws))
Bp = array(NA, c(nrow(priors$B), ncol(priors$B), no.draws))

B0.aux = B0.initial
Bp.aux = matrix(NA, nrow(priors$B), ncol(priors$B))

for (j in 1:S) {
  
   # Set the priors
  kappa1 <- 0.01      # autoregressive slope shrinkage
  kappa2 <- 1         # constant term shrinkage 
  kappa3 <- scale     # contemporaneous effects shrinkage
  kappa4 <- 1         # VAR prior persistence

  priors <- list(
  B = cbind(rep(0, N), kappa4 * diag(N), matrix(0, N, (p - 1) * N)),
  Omega = diag(c(kappa2, kappa1 * ((1:p)^(-2)) %x% rep(1, N))),
  S = kappa3 * diag(N),
  nu = shape
)
  
  # Compute posterior distribution parameters
  Omega.inv <- solve(priors$Omega)
  Omega.post.inv <- (X %*% t(X) + Omega.inv)
  Omega.post <- solve(Omega.post.inv)
  B.post <- (Y %*% t(X) + priors$B %*% Omega.inv) %*% Omega.post
  S.post <- solve(Y %*% t(Y) + solve(priors$S) + priors$B %*% Omega.inv %*% t(priors$B) - B.post %*% Omega.post.inv %*% t(B.post) +  matrix(scale^(-1), nrow = N, ncol = N))
  nu.post <- nrow(y) + priors$nu 

  # Check if Omega.post and S.post is positive definite and store the inverse if it is
  is_positive_definite <- tryCatch({
    chol(Omega.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  is_positive_definite <- tryCatch({
    chol(S.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  S.inv <- solve(S.post)
  nu = nu.post
  
  
  for (n in 1:N) {
    rn = nrow(V[[n]])
    Un = chol(nu * solve(V[[n]] %*% S.inv %*% t(V[[n]])))
    w = t(orthogonal.complement.matrix.TW(t(B0.aux[-n, ])))
    w1 = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
    if (rn > 1) {
      Wn = cbind(t(w1), orthogonal.complement.matrix.TW(t(w1)))
    } else {
      Wn = w1
    }
    alpha = rep(NA, rn)
    u = rmvnorm(1, rep(0, nu + 1), (1 / nu) * diag(nu + 1))
    alpha[1] = sqrt(as.numeric(u %*% t(u)))
    if (runif(1) < 0.5) {
      alpha[1] = -alpha[1]
    }
    if (rn > 1) {
      alpha[2:rn] = rmvnorm(1, rep(0, nrow(V[[n]]) - 1), (1 / nu) * diag(rn - 1))
    }
    bn = alpha %*% Wn %*% Un
    B0.aux[n, ] = bn %*% V[[n]]

    K = ncol(B.post)
    L = t(chol(Omega.post))

    Bp.aux[n, ] = as.vector(t(B0.aux[n, ] %*% B.post) + L %*% rnorm(K))
  }

  B0[ , , j] = B0.aux
  Bp[ , , j] = Bp.aux


  total       = 0 
  
  for (j in 1:ncol(Y)) {
    
    A = Y[, j]
    A = matrix(A, ncol = 1)
    B = X[, j]
    B = matrix(B, ncol = 1)
    C = (B0.aux %*% A - Bp.aux %*% B)

    total = total + t(C) %*% C
    
  }
  
    scale = total # This is the scale parameter
    scale = scale[1, 1] # Converting scale into a scalar

}

B0_posterior <- B0
Bp_posterior <- Bp

# Normalization
B0_hat <- diag(sign(diag(B0_posterior[, , S]))) %*% B0_posterior[, , S]

B0_posterior <- normalize.Gibbs.output.parallel(B0_posterior, B0.hat = B0_hat)

# Sample B+ from the normal conditional posterior
#Bp_posterior2 <- Bp_posterior 
Bp_posterior <- rnorm.ngn(B0.posterior = B0_posterior, B = B.post, Omega = Omega.post)

return(list(B0_posterior = B0_posterior, Bp_posterior = Bp_posterior))

}

results <- estimate_extension_I_model(S_burnin, S)

B0_posterior2 <- results$B0_posterior
Bp_posterior2 <- results$Bp_posterior

```

```{r defining the Vn matrices for the exclusion restrictions on B0 for the Extension  II model, echo=FALSE, eval=FALSE} 

# Create a list V with predefined fixed matrices
create_V_matrices <- function() {
  V <- list()
  V[[1]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0,
                     0, 0, 0, 1), nrow = 4, ncol = 4, byrow = TRUE)
  V[[2]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0,
                     0, 0, 0, 1), nrow = 4, ncol = 4, byrow = TRUE)
  V[[3]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0,
                     0, 0, 0, 1), nrow = 4, ncol = 4, byrow = TRUE)
  V[[4]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0,
                     0, 0, 0, 1), nrow = 4, ncol = 4, byrow = TRUE)
  return(V)
}

# Generate the list of V matrices
V_matrices <- create_V_matrices()

```



```{r calculating the standard deviation for the Extension I B0 & Bp matrices, echo=FALSE, eval=TRUE}

calculate_and_print_B0sd(B0_posterior2)

calculate_and_print_Bpsd(Bp_posterior2)

```

```{r plots for the Extension I model, echo=FALSE, eval=TRUE}

plot_lagged_effects(B0_posterior2, Bp_posterior2)

```


```{r printing B0 & Bp for the Extension I Model, echo=FALSE, eval=TRUE}

print("Extension I Model Normalized B0 matrix")
print(B0_posterior2[ , , S])

print("Extension I Model B plus matrix")
print(Bp_posterior2[ , , S])

```


```{r calculate the significance levels for the Extension I model B0 and Bp, echo=FALSE, eval=TRUE}

# Calculate significance for B0 and Bp matrices
B0_significance <- calculate_significance(B0_posterior2)
Bp_significance <- calculate_significance(Bp_posterior2)

# Print significance matrices
print("Significance for B0 Matrix:")
print(B0_significance)
#print("Significance for Bp Matrix:")
#print(Bp_significance)


```

```{r Plotting the IRF response for the EXtension I model, echo=FALSE, eval=TRUE}

B0 <- B0_posterior2
Bp <- Bp_posterior2

plot_irfs(B0, Bp, horizon = 10, conf_level = 0.95)

```

### Second Extension: Estimating the scale of $\lambda$

Following @Lee2022 we use a Metropolis-Hastings Algorithm to estimate $\nu$ and impose a vmode value corresponding to the t-distribution range $(0, 25)$. 


```{r this code uses a Metropolis-Hastings Algorithm to find the value of the prior nu, echo=FALSE, eval=TRUE}

# Function to sample from posterior of degrees of freedom (DoF) for the structural Bayesian SVAR
sample_nu_svar <- function(y, mu, Sigma, initial_nu, iterations, proposal_sd) {
  
  # Nested posterior function: Posterior log density of nu for SVAR
  log_posterior_nu <- function(nu, y, mu, Sigma) {
    
    # Convert matrix y to ensure correct dimensions for the following operation 
    
  # Adjusting n and d based on the transposed structure
  n <- nrow(y)  # Number of rows represents the number of observations
  d <- ncol(y)  # Number of columns represents the number of variables

    
    log_lik <- -n * lgamma(nu / 2) + (nu / 2) * sum(log(diag(Sigma)))
    for (i in 1:n) {
      res <- y[i, ] - mu
      log_lik <- log_lik - (nu + d) / 2 * log(1 + t(res) %*% solve(Sigma) %*% res / nu)
    }
    log_prior <- 0
    return(log_lik + log_prior)
  }

  # Initialize the chain
  nu <- initial_nu
  samples <- numeric(iterations)

  # Metropolis-Hastings Algorithm: 
  for (iter in 1:iterations) {
    proposed_nu <- nu + rnorm(1, 0, proposal_sd)
    if (proposed_nu <= 0) next

    log_acceptance_ratio <- log_posterior_nu(proposed_nu, y, mu, Sigma) - log_posterior_nu(nu, y, mu, Sigma)
    if (log(runif(1)) < log_acceptance_ratio) {
      nu <- proposed_nu
    }

    samples[iter] <- nu
  }

  return(samples)
}

```


```{r calculate nu for the Extension II Bayesian SVAR model, echo=FALSE, eval=TRUE}

  matrices <- create_matrices()
  Y = matrices$Y
  y = Y
 
# compute posterior distribution parameters, matrices V, and starting values
############################################################


mu <- colMeans(y)  # Mean for each variable
Sigma <- cov(y)  # Compute covariance matrix of transposed y


# Check the dimensions of the resulting matrix
print(dim(Sigma))


# Adjust the proposal_sd based on the data variance
variable_sd <- apply(y, 2, sd)  # Standard deviation for each column (variable)
proposal_sd_adjusted <- mean(variable_sd) / 10  # Adjusted proposal standard deviation

initial_nu <- 5 # To ensure the fourth moment exists. 

print("initial_nu")
print(initial_nu)

iterations <- 2000

# Sample DoF for Bayesian SVAR model
nu_samples <- sample_nu_svar(y, mu, Sigma, initial_nu, iterations, proposal_sd_adjusted)

# Analyze results
summary(nu_samples)
hist(nu_samples, main = "Posterior Distribution of Degrees of Freedom", xlab = "Degrees of Freedom (ν)")

# Choose the mode nu from the histogram
# Create histogram of the samples
hist_data <- hist(nu_samples, breaks = 30, plot = FALSE)
# Find the bin with the maximum count
selected_nu_mode <- hist_data$mids[which.max(hist_data$counts)]
print(selected_nu_mode)

```

```{r calculate B0 and B+ for the Extension II Bayesian SVAR model, echo=FALSE, eval=TRUE}

estimate_extension_II_model <- function (S_burnin, S){
  
  matrices <- create_matrices()
  Y = matrices$Y

  y   = Y
   
  # Basic settings and model dimensions
  N <- ncol(y)
  p <- 12   # This is the number of lags in the model
  K <- 1 + N * p 

# create Y and X
############################################################
Y       = y[(p+1):nrow(y),]
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[((p+1):nrow(y))-i,])
}

Y       = t(Y)
X       = t(X)


 V <- list()
 
  V[[1]] <- matrix(c(1, 0, 0, 0), nrow = 1, ncol = 4, byrow = TRUE)
  V[[2]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0), nrow = 2, ncol = 4, byrow = TRUE)
  V[[3]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0), nrow = 3, ncol = 4, byrow = TRUE)
  V[[4]] <- matrix(c(1, 0, 0, 0,
                     0, 1, 0, 0,
                     0, 0, 1, 0,
                     0, 0, 0, 1), nrow = 4, ncol = 4, byrow = TRUE)

# Set the priors
kappa1 <- 0.01      # autoregressive slope shrinkage
kappa2 <- 1         # constant term shrinkage 
kappa3 <- 1000      # contemporaneous effects shrinkage
kappa4 <- 1         # VAR prior persistence

# Basic settings and model dimensions
N <- ncol(y)
p <- 12   # This is the number of lags in the model
K <- 1 + N * p

priors <- list(
  B = cbind(rep(0, N), kappa4 * diag(N), matrix(0, N, (p - 1) * N)),
  Omega = diag(c(kappa2, kappa1 * ((1:p)^(-2)) %x% rep(1, N))),
  S = kappa3 * diag(N),
  nu = sample(nu_samples, size = 1)
)

# Initialize B0
B0.initial <- matrix(0, N, N)
for (n in 1:N) {
  unrestricted <- apply(V[[n]], 2, sum) == 1
  B0.initial[n, unrestricted] <- rnorm(sum(unrestricted))
}

N = nrow(B0.initial)
no.draws = S_burnin

B0 = array(NA, c(N, N, no.draws))
Bp = array(NA, c(nrow(priors$B), ncol(priors$B), no.draws))

B0.aux = B0.initial
Bp.aux = matrix(NA, nrow(priors$B), ncol(priors$B))

# Select the first value for lambda 
shape <- 7
scale <- 5

for (i in 1:S_burnin) {
  
  # Set the priors
  kappa1 <- 0.01      # autoregressive slope shrinkage
  kappa2 <- 1         # constant term shrinkage 
  kappa3 <- scale     # contemporaneous effects shrinkage
  kappa4 <- 1         # VAR prior persistence

  priors <- list(
  B = cbind(rep(0, N), kappa4 * diag(N), matrix(0, N, (p - 1) * N)),
  Omega = diag(c(kappa2, kappa1 * ((1:p)^(-2)) %x% rep(1, N))),
  S = kappa3 * diag(N),
  nu =  sample(nu_samples, size = 1)
)
  
  # Compute posterior distribution parameters
  Omega.inv <- solve(priors$Omega)
  Omega.post.inv <- (X %*% t(X) + Omega.inv)
  Omega.post <- solve(Omega.post.inv)
  B.post <- (Y %*% t(X) + priors$B %*% Omega.inv) %*% Omega.post
  S.post <- solve(Y %*% t(Y) + solve(priors$S) + priors$B %*% Omega.inv %*% t(priors$B) - B.post %*% Omega.post.inv %*% t(B.post))
  nu.post <- nrow(y) + priors$nu  
  
  # Check if Omega.post and S.post is positive definite and store the inverse if it is
  is_positive_definite <- tryCatch({
    chol(Omega.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  is_positive_definite <- tryCatch({
    chol(S.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }
  
  S.inv <- solve(S.post)
  nu = nu.post
  

  for (n in 1:nrow(B0.initial)) {
    rn = nrow(V[[n]])
    Un = chol(nu * solve(V[[n]] %*% S.inv %*% t(V[[n]])))
    w = t(orthogonal.complement.matrix.TW(t(B0.aux[-n, ])))
    w1 = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
    if (rn > 1) {
      Wn = cbind(t(w1), orthogonal.complement.matrix.TW(t(w1)))
    } else {
      Wn = w1
    }
    alpha = rep(NA, rn)
    u = rmvnorm(1, rep(0, nu + 1), (1 / nu) * diag(nu + 1))
    alpha[1] = sqrt(as.numeric(u %*% t(u)))
    if (runif(1) < 0.5) {
      alpha[1] = -alpha[1]
    }
    if (rn > 1) {
      alpha[2:rn] = rmvnorm(1, rep(0, nrow(V[[n]]) - 1), (1 / nu) * diag(rn - 1))
    }
    bn = alpha %*% Wn %*% Un
    B0.aux[n, ] = bn %*% V[[n]]
    
    K = ncol(B.post)
    L = t(chol(Omega.post))
    
    Bp.aux[n, ] = as.vector(t(B0.aux[n, ] %*% B.post) + L %*% rnorm(K))
  }
  
  B0[, , i] = B0.aux
  Bp[, , i] = Bp.aux


  total       = 0 
  
  for (j in 1:ncol(Y)) {
    
    A = Y[, j]
    A = matrix(A, ncol = 1)
    B = X[, j]
    B = matrix(B, ncol = 1)
    C = (B0.aux %*% A - Bp.aux %*% B)
    
    total = total + t(C) %*% C
    
  }
  

    scale = total          # This is the scale parameter
    scale = scale[1, 1]    # Converting scale into a scalar
  
}

B0_posterior <- B0

# Initialize B0 again for the remainder of the iterations 
B0.initial <- B0_posterior[, , S_burnin]
 
N = nrow(B0.initial)
no.draws = S

B0 = array(NA, c(N, N, no.draws))
Bp = array(NA, c(nrow(priors$B), ncol(priors$B), no.draws))

B0.aux = B0.initial
Bp.aux = matrix(NA, nrow(priors$B), ncol(priors$B))

for (j in 1:S) {
  
   # Set the priors
  kappa1 <- 0.01      # autoregressive slope shrinkage
  kappa2 <- 1         # constant term shrinkage 
  kappa3 <- scale     # contemporaneous effects shrinkage
  kappa4 <- 1         # VAR prior persistence

  priors <- list(
  B = cbind(rep(0, N), kappa4 * diag(N), matrix(0, N, (p - 1) * N)),
  Omega = diag(c(kappa2, kappa1 * ((1:p)^(-2)) %x% rep(1, N))),
  S = kappa3 * diag(N),
  nu = sample(nu_samples, size = 1)
)
  
  # Compute posterior distribution parameters
  Omega.inv <- solve(priors$Omega)
  Omega.post.inv <- (X %*% t(X) + Omega.inv)
  Omega.post <- solve(Omega.post.inv)
  B.post <- (Y %*% t(X) + priors$B %*% Omega.inv) %*% Omega.post
  S.post <- solve(Y %*% t(Y) + solve(priors$S) + priors$B %*% Omega.inv %*% t(priors$B) - B.post %*% Omega.post.inv %*% t(B.post))
  nu.post <- nrow(y) + priors$nu 

  # Check if Omega.post and S.post is positive definite and store the inverse if it is
  is_positive_definite <- tryCatch({
    chol(Omega.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  is_positive_definite <- tryCatch({
    chol(S.post)
    TRUE
  }, error = function(e) {
    FALSE
  })

  if (!is_positive_definite) {
    next  # Skip to the next iteration if S.post is not positive definite
  }

  S.inv <- solve(S.post)
  nu = nu.post
  
  
  for (n in 1:N) {
    rn = nrow(V[[n]])
    Un = chol(nu * solve(V[[n]] %*% S.inv %*% t(V[[n]])))
    w = t(orthogonal.complement.matrix.TW(t(B0.aux[-n, ])))
    w1 = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
    if (rn > 1) {
      Wn = cbind(t(w1), orthogonal.complement.matrix.TW(t(w1)))
    } else {
      Wn = w1
    }
    alpha = rep(NA, rn)
    u = rmvnorm(1, rep(0, nu + 1), (1 / nu) * diag(nu + 1))
    alpha[1] = sqrt(as.numeric(u %*% t(u)))
    if (runif(1) < 0.5) {
      alpha[1] = -alpha[1]
    }
    if (rn > 1) {
      alpha[2:rn] = rmvnorm(1, rep(0, nrow(V[[n]]) - 1), (1 / nu) * diag(rn - 1))
    }
    bn = alpha %*% Wn %*% Un
    B0.aux[n, ] = bn %*% V[[n]]

    K = ncol(B.post)
    L = t(chol(Omega.post))

    Bp.aux[n, ] = as.vector(t(B0.aux[n, ] %*% B.post) + L %*% rnorm(K))
  }

  B0[ , , j] = B0.aux
  Bp[ , , j] = Bp.aux


  total       = 0 
  
  for (j in 1:ncol(Y)) {
    
    A = Y[, j]
    A = matrix(A, ncol = 1)
    B = X[, j]
    B = matrix(B, ncol = 1)
    C = (B0.aux %*% A - Bp.aux %*% B)

    total = total + t(C) %*% C
    
  }
  
    scale = total # This is the scale parameter
    scale = scale[1, 1] # Converting scale into a scalar

}

B0_posterior <- B0
Bp_posterior <- Bp

# Normalization
B0_hat <- diag(sign(diag(B0_posterior[, , S]))) %*% B0_posterior[, , S]

B0_posterior <- normalize.Gibbs.output.parallel(B0_posterior, B0.hat = B0_hat)

# Sample B+ from the normal conditional posterior
#Bp_posterior2 <- Bp_posterior 
Bp_posterior <- rnorm.ngn(B0.posterior = B0_posterior, B = B.post, Omega = Omega.post)

return(list(B0_posterior = B0_posterior, Bp_posterior = Bp_posterior))

}

results <- estimate_extension_II_model(S_burnin, S)

B0_posterior3 <- results$B0_posterior
Bp_posterior3 <- results$Bp_posterior

```

```{r print the B0 & BP matrices for the Extension II model, echo=FALSE, eval=TRUE}

print("Extension II Model Normalized B0 matrix")
print(B0_posterior3[,,S])

print("Extension II Model B plus matrix")
print(Bp_posterior3[,,S])

```

```{r calculating the standard deviation for the Extension II model B0 matrix, echo=FALSE, eval=TRUE}

calculate_and_print_B0sd(B0_posterior3)

```

```{r calculating the standard deviation for the Extension II model Bp matrix, echo=FALSE, eval=TRUE}

calculate_and_print_Bpsd(Bp_posterior3)

```

```{r plot lagged Bp for the Extension II model, echo=FALSE, eval=TRUE}

plot_lagged_effects(B0_posterior3, Bp_posterior3)

```

```{r calculate the significance levels for the Extension II B0 and Bp, echo=FALSE, eval=TRUE}

# Calculate significance for B0 and Bp matrices
B0_significance <- calculate_significance(B0_posterior3)
Bp_significance <- calculate_significance(Bp_posterior3)

# Print significance matrices
print("Significance for B0 Matrix:")
print(B0_significance)
print("Significance for Bp Matrix:")
print(Bp_significance)


```


```{r Plotting the IRF response for the Extension II model, echo=FALSE, eval=TRUE}

B0 <- B0_posterior3
Bp <- Bp_posterior3

plot_irfs(B0, Bp, horizon = 10, conf_level = 0.95)

```

### Conclusion

The overall kernel for this extended model encapsulates to the random variance $\lambda$. This allows for a more robust handling of heteroscedasticity and potential non-stationarities in the error terms, offering nuanced insights into the dynamics under study.

In the basic model, all USD Index contemporaneous coefficients are not statistically significant at the standard levels. On the other hand, all these coefficients in the extended model are statistically significant at the 5% level. 

The results suggest the following elasticities. According to the basic model, a weaker Dixie is likely to overheat the Australian economy: a drop of 1 per cent in the USD Index would result in roughly 1 per cent increase in inflation, roughly unchanged unemployment, and roughly 1% increase in the GDP growth rate. 

On the other hand, the expanded model suggests that a weakening Dixie would result in an untraditional condition in the Australian economy: 1 per cent decrease in the USD Index would result is roughly 0.7 per cent increase in inflation, roughly 1 per cent increase in unemployment, and roughly 1 per cent increase in the GDP growth rate. 

As to the second extension, where $\nu$ is a hyperparameter, this model suggests that a weakening Dixie would result in stagflation in Australia: 1 per cent decrease in the USD Index would result is roughly 3 per cent increase in inflation, roughly 0.7 per cent increase in unemployment, and roughly 3 per cent decrease in the GDP growth rate. 


```{r furnish the results for a 1% drop of the USD_Index in a table, echo=FALSE, eval=TRUE}

# Load the knitr package for better table formatting
library(knitr)

# Initialize empty lists to store the results
basic_model_results <- list()
extended_model_results <- list()
second_extension_results <- list()

# Loop through each economic variable
for (i in 2:4) {
  # Calculate the effect for the Basic model
  basic_effect <- B0_posterior1[i, i, S] / B0_posterior1[i, 1, S] 
  basic_model_results[[i - 1]] <- basic_effect
  
  # Calculate the effect for the Extended model
  extended_effect <-   B0_posterior2[i, i, S] / B0_posterior2[i, 1, S] 
  extended_model_results[[i - 1]] <- extended_effect
  
  # Calculate the effect for the Second Extension model
  second_extension_effect <-   B0_posterior3[i, i, S] / B0_posterior3[i, 1, S] 
  second_extension_results[[i - 1]] <- second_extension_effect
}

# Create a data frame to hold the results
results_df <- data.frame(
  Variable = c("Inflation", "Unemployment", "GDP Growth"),
  Basic_Model = unlist(basic_model_results),
  Extended_Model = unlist(extended_model_results),
  Second_Extension = unlist(second_extension_results)
)

# Print the results table using kable for better formatting
kable(results_df, format = "html", caption = "Effect of a weaker USD_Index on Economic Indicators (in elasticities)", col.names = c("Variable (elasticity w.r.t. USD_Index)", "Basic Model", "Extended Model", "Second Extension Model"), align = "c")

```










